# -*- coding: utf-8 -*-
"""assignment3_sample.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NFs71CUOpaxReVljwznKJOTSxqd-Hi-A
"""

# ! pip install transformers
# ! pip install datasets==2.21.0
# ! pip install
# ! pip install tqdm
# ! pip install
# ! pip install torchmetrics
from datasets import load_dataset
import torch
from torch.utils.data import Dataset, DataLoader
import transformers as T
from torch.optim import AdamW
from tqdm import tqdm
from torchmetrics import SpearmanCorrCoef, Accuracy, F1Score

# 有些中文的標點符號在tokenizer編碼以後會變成[UNK]，所以將其換成英文標點
token_replacement = [
    ["：" , ":"],
    ["，" , ","],
    ["“" , "\""],
    ["”" , "\""],
    ["？" , "?"],
    ["……" , "..."],
    ["！" , "!"]
]

class SemevalDataset(Dataset):
    def __init__(self, split="train") -> None:
        super().__init__()
        assert split in ["train", "validation", "test"]
        self.data = load_dataset(
            "sem_eval_2014_task_1", split=split, cache_dir="./cache/", trust_remote_code=True
        ).to_list()

    def __getitem__(self, index):
        d = self.data[index]
        # 把中文標點替換掉
        for k in ["premise", "hypothesis"]:
            for tok in token_replacement:
                d[k] = d[k].replace(tok[0], tok[1])
        return d

    def __len__(self):
        return len(self.data)

# data_sample = SemevalDataset(split="train").data

# for i, sample in enumerate(data_sample):
#     if sample['entailment_judgment'] == 2:
#         print(f"Dataset example {i}: \n{sample}")

# Define the hyperparameters
lr =  1e-5
epochs = 10
train_batch_size = 8
validation_batch_size = 8

# TODO1: Create batched data for DataLoader
# `collate_fn` is a function that defines how the data batch should be packed.
# This function will be called in the DataLoader to pack the data batch.
tokenizer = T.BertTokenizer.from_pretrained("google-bert/bert-base-uncased", cache_dir="./cache/")

def collate_fn(batch):
    premises = [item['premise'] for item in batch]
    hypotheses = [item['hypothesis'] for item in batch]
    relatedness_scores = torch.tensor([item['relatedness_score'] for item in batch], dtype=torch.float32)
    entailment_judgments = torch.tensor([item['entailment_judgment'] for item in batch], dtype=torch.long)
    inputs = tokenizer(premises, hypotheses, padding=True, truncation=True, return_tensors="pt")
    return {
        'input_ids': inputs['input_ids'],
        'token_type_ids': inputs['token_type_ids'],
        'attention_mask': inputs['attention_mask'],
        'relatedness_score': relatedness_scores,
        'entailment_judgment': entailment_judgments
        }
    # TODO1-1: Implement the collate_fn function
    # Write your code here
    # The input parameter is a data batch (tuple), and this function packs it into tensors.
    # Use tokenizer to pack tokenize and pack the data and its corresponding labels.
    # Return the data batch and labels for each sub-task.

# TODO1-2: Define your DataLoader
dataset_train = SemevalDataset(split="train")
dataset_val = SemevalDataset(split="validation")
dl_train = DataLoader(dataset_train, batch_size=train_batch_size, shuffle=True, collate_fn=collate_fn)
dl_validation = DataLoader(dataset_val, batch_size=validation_batch_size, shuffle=True, collate_fn=collate_fn)

print(next(iter(dl_train)))

# TODO2: Construct your model
import torch.nn as nn
class MultiLabelModel(torch.nn.Module):
    def __init__(self, model_name = "google-bert/bert-base-uncased"):
        super(MultiLabelModel, self).__init__()
        # Write your code here
        # Define what modules you will use in the model
        self.bert = T.BertModel.from_pretrained(model_name)
        self.relatedness_score_layer = nn.Linear(self.bert.config.hidden_size, 1)
        self.entailment_judgment_layer = nn.Linear(self.bert.config.hidden_size, 3)

    def forward(self, input_ids, attention_mask, token_type_ids):
        # Get the outputs from BERT
        bert_output = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids)

        cls_output = bert_output.pooler_output
        relatedness_score = self.relatedness_score_layer(cls_output)
        entailment_judgment = self.entailment_judgment_layer(cls_output)

        return {
            "relatedness_score": relatedness_score,
            "entailment_judgment": entailment_judgment
        }
        # Write your code here
        # Forward pass

device = "cuda:0" if torch.cuda.is_available() else "cpu"
print(f"Device: {device}")
model = MultiLabelModel("google-bert/bert-base-uncased").to(device)

output = next(iter(dl_train))
print(output["relatedness_score"].size(), output["entailment_judgment"].size())

optimizer = AdamW(model.parameters(), lr=lr) # Write your code here

loss_fn_relatedness = nn.MSELoss()
loss_fn_entailment = nn.CrossEntropyLoss()

spc = SpearmanCorrCoef()
acc = Accuracy(task="multiclass", num_classes=3)
f1 = F1Score(task="multiclass", num_classes=3, average='macro')

for ep in range(epochs):
    pbar = tqdm(dl_train)
    pbar.set_description(f"Training epoch [{ep+1}/{epochs}]")
    model.train()
    total_train_loss = 0
    for batch in pbar:
        optimizer.zero_grad()
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        token_type_ids = batch['token_type_ids'].to(device)
        relatedness_scores = batch['relatedness_score'].to(device)
        entailment_judgments = batch['entailment_judgment'].to(device)
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)

        loss_relatedness = loss_fn_relatedness(outputs["relatedness_score"], relatedness_scores.view(-1, 1))
        loss_entailment = loss_fn_entailment(outputs["entailment_judgment"], entailment_judgments)

        loss = 0.5* loss_relatedness + 0.5*loss_entailment
        total_train_loss += loss.item()
        loss.backward()
        optimizer.step()
        pbar.set_postfix(loss=total_train_loss / (pbar.n + 1))

    pbar = tqdm(dl_validation)
    pbar.set_description(f"Validation epoch [{ep+1}/{epochs}]")
    model.eval()
    total_val_loss = 0
    spc_score = 0
    acc_score = 0
    f1_score = 0
    with torch.no_grad():
        for batch in pbar:
            # Move batch to device
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            token_type_ids = batch['token_type_ids'].to(device)
            relatedness_scores = batch['relatedness_score'].to(device)
            entailment_judgments = batch['entailment_judgment'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)

            loss_relatedness = loss_fn_relatedness(outputs["relatedness_score"], relatedness_scores.view(-1, 1))
            loss_entailment = loss_fn_entailment(outputs["entailment_judgment"], entailment_judgments)
            val_loss = loss_relatedness + loss_entailment
            total_val_loss += val_loss.item()

            spc_score += spc(outputs["relatedness_score"].cpu(), relatedness_scores.cpu().view(-1, 1))
            acc_score += acc(outputs["entailment_judgment"].cpu(), entailment_judgments.cpu())
            f1_score += f1(outputs["entailment_judgment"].cpu(), entailment_judgments.cpu())

        avg_val_loss = total_val_loss / len(dl_validation)
        avg_spc = spc_score / len(dl_validation)
        avg_acc = acc_score / len(dl_validation)
        avg_f1 = f1_score / len(dl_validation)

        print(f"Validation Loss: {avg_val_loss:.4f} | \nSpearman Corr: {avg_spc:.4f} | \nAccuracy: {avg_acc:.4f} | \nF1 Score: {avg_f1:.4f}")

    # Save model checkpoint
    torch.save(model.state_dict(), f'./saved_models/ep{ep}.pt')

import matplotlib.pyplot as plt
import numpy as np
##(Ploting code is generate with ChatGPT)
# Training Loss data
train_loss = [0.739, 0.336, 0.256, 0.192, 0.157, 0.126, 0.102, 0.0815, 0.0715, 0.0723]

# Validation Loss data
val_loss = [0.7967, 0.6408, 0.7538, 0.6536, 0.7811, 0.7361, 0.7281, 0.7329, 0.7807, 0.8270]

# Other metrics data
spearman = [0.7597, 0.7802, 0.7687, 0.7849, 0.7898, 0.7698, 0.7833, 0.7266, 0.7568, 0.7689]
accuracy = [0.8333, 0.8571, 0.8373, 0.8492, 0.8333, 0.8552, 0.8651, 0.8750, 0.8651, 0.8492]
f1_score = [0.7792, 0.8204, 0.7984, 0.8148, 0.8046, 0.8132, 0.8290, 0.8456, 0.8297, 0.7859]

# Create figure with three subplots
plt.figure(figsize=(20, 6))

# Plot 1: Training and Validation Loss
plt.subplot(1, 2, 1)
epochs = range(1, 11)
plt.plot(epochs, train_loss, 'g-', marker='o', linewidth=2, markersize=8, label='Training Loss')
plt.plot(epochs, val_loss, 'r-', marker='o', linewidth=2, markersize=8, label='Validation Loss')
plt.title('Training vs Validation Loss', fontsize=12)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
# Plot 3: Other Metrics
plt.subplot(1, 2, 2)
plt.plot(epochs, spearman, 'r-', marker='o', label='Spearman Correlation', linewidth=2, markersize=8)
plt.plot(epochs, accuracy, 'g-', marker='s', label='Accuracy', linewidth=2, markersize=8)
plt.plot(epochs, f1_score, 'b-', marker='^', label='F1 Score', linewidth=2, markersize=8)
plt.title('Training Metrics over Epochs', fontsize=12)
plt.xlabel('Epoch')
plt.ylabel('Score')
plt.legend()
plt.grid(True)

# Adjust layout
plt.tight_layout()
plt.show()

"""

```
# 此內容會顯示為程式碼
```

For test set predictions, you can write perform evaluation simlar to #TODO5."""

dataset_test= SemevalDataset(split="test")
dl_test = DataLoader(dataset_test, batch_size=validation_batch_size, shuffle=True, collate_fn=collate_fn)

model.eval()
model.load_state_dict(torch.load('./saved_models/ep9.pt', map_location=device, weights_only=True), strict=True)
# 初始化累計變量
total_test_loss = 0
spc_score = 0
acc_score = 0
f1_score = 0
spearman_corr = SpearmanCorrCoef()
accuracy = Accuracy(task="multiclass", num_classes=3)
f1_score = F1Score(task="multiclass", num_classes=3, average='macro', zero_division=0)

with torch.no_grad():
    for batch in tqdm(dl_test, desc="Testing"):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        token_type_ids = batch['token_type_ids'].to(device)
        relatedness_scores = batch['relatedness_score'].to(device)
        entailment_judgments = batch['entailment_judgment'].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)

        loss_relatedness = loss_fn_relatedness(outputs["relatedness_score"], relatedness_scores.view(-1, 1))
        loss_entailment = loss_fn_entailment(outputs["entailment_judgment"], entailment_judgments)
        test_loss = loss_relatedness + loss_entailment
        total_test_loss += test_loss.item()
        # Update metrics
        spearman_corr.update(outputs["relatedness_score"].cpu(), relatedness_scores.cpu().view(-1, 1))
        accuracy.update(outputs["entailment_judgment"].cpu(), entailment_judgments.cpu())
        f1_score.update(outputs["entailment_judgment"].cpu(), entailment_judgments.cpu())

# Compute average loss and final metrics
average_test_loss = total_test_loss / len(dl_test)
spearman_corr_result = spearman_corr.compute()
accuracy_result = accuracy.compute()
f1_score_result = f1_score.compute()

# Print results
print(f"Test Loss: {average_test_loss:.4f} |")
print(f"Spearman Corr: {spearman_corr_result:.4f} |")
print(f"Accuracy: {accuracy_result:.4f} |")
print(f"F1 Score: {f1_score_result:.4f}")

class relatedness_score_Model(torch.nn.Module):
    def __init__(self, model_name = "google-bert/bert-base-uncased"):
        super(relatedness_score_Model, self).__init__()
        # Write your code here
        # Define what modules you will use in the model
        self.bert = T.BertModel.from_pretrained(model_name)
        self.relatedness_score_layer = nn.Linear(self.bert.config.hidden_size, 1)

    def forward(self, input_ids, attention_mask, token_type_ids):
        # Get the outputs from BERT
        bert_output = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids)

        cls_output = bert_output.pooler_output
        relatedness_score = self.relatedness_score_layer(cls_output)

        return relatedness_score

class entailment_judgment_Model(torch.nn.Module):
    def __init__(self, model_name = "google-bert/bert-base-uncased"):
        super(entailment_judgment_Model, self).__init__()
        # Write your code here
        # Define what modules you will use in the model
        self.bert = T.BertModel.from_pretrained(model_name)
        self.entailment_judgment_layer = nn.Linear(self.bert.config.hidden_size, 3)

    def forward(self, input_ids, attention_mask, token_type_ids):
        # Get the outputs from BERT
        bert_output = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids)

        cls_output = bert_output.pooler_output
        entailment_judgment = self.entailment_judgment_layer(cls_output)

        return entailment_judgment

relatedness_model = relatedness_score_Model(model_name = "google-bert/bert-base-uncased").to(device)
entailment_judgment_model = entailment_judgment_Model(model_name = "google-bert/bert-base-uncased").to(device)
optimizer_relatedness_model = AdamW(relatedness_model.parameters(), lr=lr)
optimizer_entailment_judgment_model= AdamW(entailment_judgment_model.parameters(), lr=lr)

for ep in range(epochs):
    pbar = tqdm(dl_train)
    pbar.set_description(f"Training epoch [{ep+1}/{epochs}]")
    relatedness_model.train()
    total_train_loss = 0
    for batch in pbar:
        optimizer_relatedness_model.zero_grad()
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        token_type_ids = batch['token_type_ids'].to(device)
        relatedness_scores = batch['relatedness_score'].to(device)
        outputs = relatedness_model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)

        loss_relatedness = loss_fn_relatedness(outputs, relatedness_scores.view(-1, 1))
        total_train_loss += loss_relatedness.item()
        loss_relatedness.backward()
        optimizer_relatedness_model.step()
        pbar.set_postfix(loss_relatedness=total_train_loss / (pbar.n + 1))

    pbar = tqdm(dl_validation)
    pbar.set_description(f"Validation epoch [{ep+1}/{epochs}]")
    relatedness_model.eval()
    total_val_loss = 0
    spc_score = 0
    with torch.no_grad():
        for batch in pbar:
            # Move batch to device
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            token_type_ids = batch['token_type_ids'].to(device)
            relatedness_scores = batch['relatedness_score'].to(device)

            outputs = relatedness_model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)

            loss_relatedness = loss_fn_relatedness(outputs, relatedness_scores.view(-1, 1))
            total_val_loss += loss_relatedness.item()

            spc_score += spc(outputs.cpu(), relatedness_scores.cpu().view(-1, 1))

        avg_val_loss = total_val_loss / len(dl_validation)
        avg_spc = spc_score / len(dl_validation)

        print(f"Validation Loss: {avg_val_loss:.4f} | \nSpearman Corr: {avg_spc:.4f} |\n")

for ep in range(epochs):
    pbar = tqdm(dl_train)
    pbar.set_description(f"Training epoch [{ep+1}/{epochs}]")
    entailment_judgment_model.train()
    total_train_loss = 0
    for batch in pbar:
        optimizer_entailment_judgment_model.zero_grad()
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        token_type_ids = batch['token_type_ids'].to(device)
        entailment_judgments = batch['entailment_judgment'].to(device)
        outputs = entailment_judgment_model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)

        loss_entailment = loss_fn_entailment(outputs, entailment_judgments)

        total_train_loss += loss_entailment.item()
        loss_entailment.backward()
        optimizer_entailment_judgment_model.step()
        pbar.set_postfix(loss_entailment=total_train_loss / (pbar.n + 1))

    pbar = tqdm(dl_validation)
    pbar.set_description(f"Validation epoch [{ep+1}/{epochs}]")
    entailment_judgment_model.eval()
    total_val_loss = 0
    spc_score = 0
    acc_score = 0
    f1_score = 0
    with torch.no_grad():
        for batch in pbar:
            # Move batch to device
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            token_type_ids = batch['token_type_ids'].to(device)
            entailment_judgments = batch['entailment_judgment'].to(device)

            outputs = entailment_judgment_model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)

            loss_entailment = loss_fn_entailment(outputs, entailment_judgments)
            val_loss = loss_entailment
            total_val_loss += val_loss.item()

            acc_score += acc(outputs.cpu(), entailment_judgments.cpu())
            f1_score += f1(outputs.cpu(), entailment_judgments.cpu())

        avg_val_loss = total_val_loss / len(dl_validation)
        avg_acc = acc_score / len(dl_validation)
        avg_f1 = f1_score / len(dl_validation)

        print(f"Validation Loss: {avg_val_loss:.4f} | \n\nAccuracy: {avg_acc:.4f} | \nF1 Score: {avg_f1:.4f}")

import matplotlib.pyplot as plt
##(Ploting code is generate with ChatGPT)
# Training and validation data
epochs = list(range(1, 11))
loss_relatedness = [0.783, 0.242, 0.184, 0.144, 0.114, 0.0935, 0.0819, 0.0681, 0.0599, 0.0536]
validation_loss = [0.3371, 0.2886, 0.2373, 0.2549, 0.2783, 0.2299, 0.2953, 0.2706, 0.2785, 0.2641]
spearman_corr = [0.7317, 0.7467, 0.7257, 0.7626, 0.7608, 0.7855, 0.7777, 0.7999, 0.7783, 0.7760]

# Plotting
fig, ax1 = plt.subplots(figsize=(10, 6))

# Plot Loss
ax1.plot(epochs, loss_relatedness, label='Loss (Relatedness)', color='blue', marker='o')
ax1.plot(epochs, validation_loss, label='Validation Loss', color='orange', marker='o')
ax1.set_xlabel('Epoch')
ax1.set_ylabel('Loss')
ax1.legend(loc='upper right')

# Plot Spearman Correlation on a secondary y-axis
ax2 = ax1.twinx()
ax2.plot(epochs, spearman_corr, label='Spearman Corr', color='green', marker='o')
ax2.set_ylabel('Spearman Correlation')
ax2.legend(loc='lower right')

plt.title('Training Progression: Loss and Spearman Correlation Over Epochs')
plt.show()

##(Ploting code is generate with ChatGPT)
loss_entailment = [0.608, 0.344, 0.229, 0.147, 0.104, 0.0722, 0.0589, 0.0397, 0.0358, 0.033]
validation_loss = [0.3866, 0.3465, 0.3648, 0.3961, 0.4893, 0.5191, 0.5020, 0.5299, 0.5483, 0.6675]
accuracy = [0.8492, 0.8671, 0.8651, 0.8671, 0.8750, 0.8671, 0.8710, 0.8750, 0.8651, 0.8690]
f1_score = [0.8143, 0.8238, 0.8439, 0.8183, 0.8176, 0.8185, 0.8479, 0.8489, 0.8325, 0.8227]

# Plotting
fig, ax1 = plt.subplots(figsize=(10, 6))

ax1.plot(epochs, loss_entailment, label='Loss (Entailment)', color='blue', marker='o')
ax1.plot(epochs, validation_loss, label='Validation Loss', color='orange', marker='o')
ax1.set_xlabel('Epoch')
ax1.set_ylabel('Loss')
ax1.legend(loc='upper right')

ax2 = ax1.twinx()
ax2.plot(epochs, accuracy, label='Accuracy', color='green', marker='o')
ax2.plot(epochs, f1_score, label='F1 Score', color='purple', marker='o')
ax2.set_ylabel('Accuracy / F1 Score')
ax2.legend(loc='lower right')

plt.title('Training Progression: Loss, Accuracy, and F1 Score Over Epochs')
plt.show()

