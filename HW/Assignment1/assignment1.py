# -*- coding: utf-8 -*-
"""Assignment1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pTdBA9ONQtl0yYIEnjll-KXDs15V4Kvg

## Part I: Data Pre-processing
"""

import pandas as pd

# Download the Google Analogy dataset
!wget http://download.tensorflow.org/data/questions-words.txt

# Preprocess the dataset
file_name = "questions-words"

with open(f"{file_name}.txt", "r") as f:
  data = f.read().splitlines()

# check data from the first 10 entries
for entry in data[:5]:
    print(entry)

# TODO1: Write your code here for processing data to pd.DataFrame
# Please note that the first five mentions of ": " indicate `semantic`,
# and the remaining nine belong to the `syntatic` category.
processed_data = []
category = None

semantic_categories = []
syntactic_categories = []
semantic_count = 0

for line in data:
    if line.startswith(':'):
        semantic_count += 1
        if semantic_count <= 5:
            category_type = 'Semantic'
            semantic_categories.append(line[2:])
        else:
            category_type = 'Syntactic'
            syntactic_categories.append(line[2:])
        category = line[2:]
    else:
        processed_data.append([line, category_type, category])

df = pd.DataFrame(processed_data, columns=['Question', 'Category', 'SubCategory'])
df.tail()

df.head()

df.to_csv(f"{file_name}.csv", index=False)

"""## Part II: Use pre-trained word embeddings
- After finish Part I, you can run Part II code blocks only.
"""

import pandas as pd
import numpy as np
import gensim.downloader
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

data = pd.read_csv("questions-words.csv")

MODEL_NAME =  'glove-wiki-gigaword-100'
# You can try other models.
# https://radimrehurek.com/gensim/models/word2vec.html#pretrained-models

model = gensim.downloader.load(MODEL_NAME)
print("The Gensim model loaded successfully!")

# Do predictions and preserve the gold answers (word_D)
preds = []
golds = []

for analogy in tqdm(data["Question"]):
    word_a, word_b, word_c, word_d = analogy.split()
    word_a, word_b, word_c, word_d = word_a.lower(), word_b.lower(), word_c.lower(), word_d.lower()
    try:
        predicted_word = model.most_similar(positive=[word_b, word_c], negative=[word_a], topn=1)[0][0]
    except KeyError as e:
        predicted_word = None

    preds.append(predicted_word)
    golds.append(word_d)

# 打印部分結果查看
for i in range(10,20):
    print(f"Question: {data['Question'][i]}, Predicted: {preds[i]}, Gold: {golds[i]}")

# Perform evaluations. You do not need to modify this block!!

def calculate_accuracy(gold: np.ndarray, pred: np.ndarray) -> float:
    return np.mean(gold == pred)

golds_np, preds_np = np.array(golds), np.array(preds)
data = pd.read_csv("questions-words.csv")

# Evaluation: categories
for category in data["Category"].unique():
    mask = data["Category"] == category
    golds_cat, preds_cat = golds_np[mask], preds_np[mask]
    acc_cat = calculate_accuracy(golds_cat, preds_cat)
    print(f"Category: {category}, Accuracy: {acc_cat * 100}%")

# Evaluation: sub-categories
for sub_category in data["SubCategory"].unique():
    mask = data["SubCategory"] == sub_category
    golds_subcat, preds_subcat = golds_np[mask], preds_np[mask]
    acc_subcat = calculate_accuracy(golds_subcat, preds_subcat)
    print(f"Sub-Category{sub_category}, Accuracy: {acc_subcat * 100}%")

# Collect words from Google Analogy dataset
SUB_CATEGORY = "family"

family_data = df[df['SubCategory'] == SUB_CATEGORY]

words = set()

for analogy in family_data['Question']:
    word_a, word_b, word_c, word_d = analogy.split()
    words.update([word_a, word_b, word_c, word_d])

words_in_model = [word for word in words if word in model]

word_vectors = np.array([model[word] for word in words_in_model])

tsne = TSNE(n_components=2, perplexity=5, random_state=42)
word_vectors_2d = tsne.fit_transform(word_vectors)

plt.figure(figsize=(12, 8))
plt.scatter(word_vectors_2d[:, 0], word_vectors_2d[:, 1], color='blue')

for i, word in enumerate(words_in_model):
    plt.annotate(word, xy=(word_vectors_2d[i, 0], word_vectors_2d[i, 1]), xytext=(5, 2),
                 textcoords='offset points', ha='right', va='bottom')
'''
Reference from
https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html
https://simulationbased.com/2021/01/05/introduction-to-t-sne-in-python-with-scikit-learn/
'''

plt.title("Word Relationships from Google Analogy Task")
plt.show()
plt.savefig("word_relationships.png", bbox_inches="tight")

"""### Part III: Train your own word embeddings

### Get the latest English Wikipedia articles and do sampling.
- Usually, we start from Wikipedia dump (https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2). However, the downloading step will take very long. Also, the cleaning step for the Wikipedia corpus ([`gensim.corpora.wikicorpus.WikiCorpus`](https://radimrehurek.com/gensim/corpora/wikicorpus.html#gensim.corpora.wikicorpus.WikiCorpus)) will take much time. Therefore, we provide cleaned files for you.

Please note that we used the default parameters of [`gensim.corpora.wikicorpus.WikiCorpus`](https://radimrehurek.com/gensim/corpora/wikicorpus.html#gensim.corpora.wikicorpus.WikiCorpus) for cleaning the Wiki raw file. Thus, words with one character were discarded.
"""

# Now you need to do sampling because the corpus is too big.
# You can further perform analysis with a greater sampling ratio.

import random

wiki_txt_path = "wiki_texts_combined.txt"
output_path ="output_text.txt"
# wiki_texts_combined.txt is a text file separated by linebreaks (\n).
# Each row in wiki_texts_combined.txt indicates a Wikipedia article.

with open(wiki_txt_path, "r", encoding="utf-8") as f:
  line = f.readlines()
  sample_size = int(0.5 * len(line))
  sampled_lines = random.sample(line, sample_size)
  with open(output_path, "w", encoding="utf-8") as output_file:
    output_file.writelines(sampled_lines)

# TODO4: Sample `20%` Wikipedia articles
# Write your code here

# TODO5: Train your own word embeddings with the sampled articles
# https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec
# Hint: You should perform some pre-processing before training.
import multiprocessing
from gensim.models.word2vec import LineSentence
from gensim.models import Word2Vec

sentences = LineSentence('output_text.txt')

model = Word2Vec(sentences, vector_size =100, window=5, min_count=5,
                 workers=multiprocessing.cpu_count())
model.save('TODO5')

data = pd.read_csv("questions-words.csv")

# Do predictions and preserve the gold answers (word_D)
preds = []
golds = []

for analogy in tqdm(data["Question"]):
    word_a, word_b, word_c, word_d = analogy.split()
    word_a, word_b, word_c, word_d = word_a.lower(), word_b.lower(), word_c.lower(), word_d.lower()
    if all(word in model.wv for word in [word_a, word_b, word_c]):
        predicted_vector = model.wv[word_b] + model.wv[word_c] - model.wv[word_a]
        predicted_word = model.wv.most_similar(positive=[predicted_vector], topn=1)[0][0]
        preds.append(predicted_word)
    else:
        preds.append(None)
    golds.append(word_d)
for i in range(10):
    print(f"Analogy: {data['Question'][i]}, Predicted: {preds[i]}, Gold: {golds[i]}")

""" Hints
  # Unpack the analogy (e.g., "man", "woman", "king", "queen")
  # Perform vector arithmetic: word_b + word_c - word_a should be close to word_d
  # Source: https://github.com/piskvorky/gensim/blob/develop/gensim/models/keyedvectors.py#L776
  # Mikolov et al., 2013: big - biggest and small - smallest
  # Mikolov et al., 2013: X = vector(”biggest”) − vector(”big”) + vector(”small”).
"""

golds_np, preds_np = np.array(golds), np.array(preds)
data = pd.read_csv("questions-words.csv")
def calculate_accuracy(gold: np.ndarray, pred: np.ndarray) -> float:
    return np.mean(gold == pred)
# Evaluation: categories
for category in data["Category"].unique():
    mask = data["Category"] == category
    golds_cat, preds_cat = golds_np[mask], preds_np[mask]
    acc_cat = calculate_accuracy(golds_cat, preds_cat)
    print(f"Category: {category}, Accuracy: {acc_cat * 100}%")

# Evaluation: sub-categories
for sub_category in data["SubCategory"].unique():
    mask = data["SubCategory"] == sub_category
    golds_subcat, preds_subcat = golds_np[mask], preds_np[mask]
    acc_subcat = calculate_accuracy(golds_subcat, preds_subcat)
    print(f"Sub-Category{sub_category}, Accuracy: {acc_subcat * 100}%")

# Collect words from Google Analogy dataset
SUB_CATEGORY = "family"

# TODO7: Plot t-SNE for the words in the SUB_CATEGORY `: family`
family_data = data[data['SubCategory'] == SUB_CATEGORY]

words = set()

for analogy in family_data['Question']:
    word_a, word_b, word_c, word_d = analogy.split()
    words.update([word_a, word_b, word_c, word_d])

words_in_model = [word for word in words if word in model.wv]

word_vectors = np.array([model.wv[word] for word in words_in_model])

tsne = TSNE(n_components=2, perplexity=5, random_state=42)
word_vectors_2d = tsne.fit_transform(word_vectors)

plt.figure(figsize=(12, 8))
plt.scatter(word_vectors_2d[:, 0], word_vectors_2d[:, 1], color='blue')

for i, word in enumerate(words_in_model):
    plt.annotate(word, xy=(word_vectors_2d[i, 0], word_vectors_2d[i, 1]), xytext=(5, 2),
                 textcoords='offset points', ha='right', va='bottom')

plt.title("Word Relationships from Google Analogy Task")
plt.show()
plt.savefig("word_relationships.png", bbox_inches="tight")

